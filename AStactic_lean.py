# -*- coding: utf-8 -*-
"""AStactic+Lean.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17gync9sCV7ijmxyvZQK7DwUgO9eTmmKd
"""

import os
import math
import csv
import json
import time
import argparse
import random
import tempfile
import subprocess
from dataclasses import dataclass
from contextlib import nullcontext
from typing import List, Optional, Tuple

import torch
import torch.nn as nn
from torch.utils.data import Dataset as TorchDataset, DataLoader
from datasets import load_from_disk


# Argument parsing

def build_args():
    p = argparse.ArgumentParser(description="Train and verify ASTactic with Lean")
    # Data / IO
    p.add_argument("--data_root", type=str, required=True, help="HF dataset root (load_from_disk)")
    p.add_argument("--save_dir", type=str, required=True, help="Directory for checkpoints and logs")
    p.add_argument("--run_name", type=str, default="run", help="Run name label for logs/artifacts")
    p.add_argument("--goal_field", type=str, default="goal")
    p.add_argument("--gold_tactic_field", type=str, default="gold_tactic")
    p.add_argument("--imports_field", type=str, default="imports")

    # Modes
    p.add_argument("--mode", type=str, default="train+verify",
                   choices=["train", "verify", "train+verify"],
                   help="Execute training, verification, or both")
    p.add_argument("--verify_splits", type=str, nargs="+", default=["validation"],
                   choices=["validation", "test"],
                   help="Splits to verify when mode includes verification")

    # Training
    p.add_argument("--epochs", type=int, default=5)
    p.add_argument("--batch_size", type=int, default=32)
    p.add_argument("--lr", type=float, default=3e-4)
    p.add_argument("--num_workers", type=int, default=4)
    p.add_argument("--grad_clip", type=float, default=1.0)
    p.add_argument("--seed", type=int, default=1337)

    # Model / lengths
    p.add_argument("--device", type=str, default="cuda")
    p.add_argument("--d_model", type=int, default=256)
    p.add_argument("--nhead", type=int, default=4)
    p.add_argument("--num_layers", type=int, default=4)
    p.add_argument("--ff_dim", type=int, default=1024)
    p.add_argument("--dropout", type=float, default=0.1)
    p.add_argument("--max_src_len", type=int, default=1024)
    p.add_argument("--max_tgt_len", type=int, default=256)
    p.add_argument("--label_smoothing", type=float, default=0.1)

    # Decoding / verification
    p.add_argument("--beam_size", type=int, default=5)
    p.add_argument("--lean_timeout", type=float, default=10.0)
    p.add_argument("--default_imports", type=str, default="Std",
                   help="Comma-separated default imports if dataset row has no 'imports'")
    # Checkpoint
    p.add_argument("--load_ckpt", type=str, default="",
                   help="Path to a checkpoint to load before verification (overrides best.pt if provided)")

    args = p.parse_args()
    return args


# Repro / CUDA settings

def set_seed(seed: int):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True


# Data pipeline

class HFTacticDataset(TorchDataset):
    """
    Byte-level encoding dataset:
    - src: encoded goal text
    - tgt: encoded tactic string (optional on eval)
    """
    PAD, BOS, EOS = 0, 257, 258

    def __init__(
        self,
        hf_split,
        goal_field="goal",
        gold_tactic_field="gold_tactic",
        imports_field="imports",
        max_src_len=1024,
        max_tgt_len=256,
    ):
        self.ds = hf_split
        self.goal_field = goal_field
        self.gold_field = gold_tactic_field
        self.imports_field = imports_field
        self.max_src_len = max_src_len
        self.max_tgt_len = max_tgt_len
        sample = self.ds[0]
        if goal_field not in sample:
            raise KeyError(f"Missing '{goal_field}' in dataset. Keys: {list(sample.keys())}")

    @staticmethod
    def _bytes_to_ids(s: str, max_len: int) -> torch.Tensor:
        b = (s or "").encode("utf-8", errors="ignore")[:max_len]
        return torch.tensor([bt + 1 for bt in b], dtype=torch.long)

    def __len__(self) -> int:
        return self.ds.num_rows

    def __getitem__(self, i: int):
        ex = self.ds[i]
        goal = ex.get(self.goal_field, "") or ""
        src_ids = self._bytes_to_ids(goal, self.max_src_len)

        gold_tac = ex.get(self.gold_field, None)
        tgt_ids = None
        if gold_tac is not None:
            raw = gold_tac.encode("utf-8", errors="ignore")[: max(1, self.max_tgt_len - 2)]
            tgt_ids = [bt + 1 for bt in raw]

        imports = ex.get(self.imports_field, None)
        return {
            "id": ex.get("id", i),
            "goal": goal,
            "imports": imports,
            "src_ids": src_ids,
            "tgt_ids": tgt_ids,
        }


def collate_tactics(batch: List[dict], max_tgt_len=256):
    PAD, BOS, EOS = 0, 257, 258
    B = len(batch)

    # Pad sources
    maxL = max(x["src_ids"].size(0) for x in batch)
    X = torch.full((B, maxL), PAD, dtype=torch.long)
    for i, x in enumerate(batch):
        X[i, : x["src_ids"].size(0)] = x["src_ids"]

    # Targets
    has_gold = all(b.get("tgt_ids") is not None for b in batch)
    if has_gold:
        tgt_seqs = []
        for b in batch:
            ids = [BOS] + b["tgt_ids"] + [EOS]
            tgt_seqs.append(torch.tensor(ids, dtype=torch.long))
        Lt = max(t.size(0) for t in tgt_seqs)
        TGT = torch.full((B, Lt), PAD, dtype=torch.long)
        for i, t in enumerate(tgt_seqs):
            TGT[i, : t.size(0)] = t
        tgt_in, tgt_out = TGT[:, :-1], TGT[:, 1:]
    else:
        tgt_in = tgt_out = None

    meta = [{"id": b["id"], "goal": b["goal"], "imports": b["imports"]} for b in batch]
    return {"src_ids": X, "tgt_in_ids": tgt_in, "tgt_out_ids": tgt_out, "meta": meta}


# Model

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 8192):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        self.register_buffer("pe", pe.unsqueeze(0), persistent=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:, : x.size(1), :]


@dataclass
class ASTacticConfig:
    src_vocab_size: int = 259
    tgt_vocab_size: int = 259
    d_model: int = 256
    nhead: int = 4
    num_layers: int = 4
    dim_feedforward: int = 1024
    dropout: float = 0.1
    max_src_len: int = 1024
    max_tgt_len: int = 256
    pad_id: int = 0
    bos_id: int = 257
    eos_id: int = 258
    label_smoothing: float = 0.1


class ASTacticModel(nn.Module):
    def __init__(self, cfg: ASTacticConfig):
        super().__init__()
        self.cfg = cfg
        self.src_embed = nn.Embedding(cfg.src_vocab_size, cfg.d_model, padding_idx=cfg.pad_id)
        self.tgt_embed = nn.Embedding(cfg.tgt_vocab_size, cfg.d_model, padding_idx=cfg.pad_id)
        self.src_pos = PositionalEncoding(cfg.d_model, cfg.max_src_len)
        self.tgt_pos = PositionalEncoding(cfg.d_model, cfg.max_tgt_len)
        self.trf = nn.Transformer(
            d_model=cfg.d_model,
            nhead=cfg.nhead,
            num_encoder_layers=cfg.num_layers,
            num_decoder_layers=cfg.num_layers,
            dim_feedforward=cfg.dim_feedforward,
            dropout=cfg.dropout,
            batch_first=True,
            norm_first=True,
        )
        self.out_proj = nn.Linear(cfg.d_model, cfg.tgt_vocab_size, bias=False)
        self.criterion = nn.CrossEntropyLoss(ignore_index=cfg.pad_id, label_smoothing=cfg.label_smoothing)

    @staticmethod
    def _causal_mask(L: int, device) -> torch.Tensor:
        return torch.triu(torch.ones(L, L, device=device, dtype=torch.bool), diagonal=1)

    def forward(self, src_ids: torch.Tensor, tgt_in_ids: torch.Tensor) -> torch.Tensor:
        device = src_ids.device
        src = self.src_pos(self.src_embed(src_ids))
        tgt = self.tgt_pos(self.tgt_embed(tgt_in_ids))
        src_key = (src_ids == self.cfg.pad_id)
        tgt_mask = self._causal_mask(tgt_in_ids.size(1), device)
        mem = self.trf.encoder(src, src_key_padding_mask=src_key)
        dec = self.trf.decoder(tgt, mem, tgt_mask=tgt_mask, memory_key_padding_mask=src_key)
        return self.out_proj(dec)

    def loss(self, logits: torch.Tensor, tgt_out_ids: torch.Tensor) -> torch.Tensor:
        B, Lt, V = logits.shape
        return self.criterion(logits.view(B * Lt, V), tgt_out_ids.reshape(B * Lt))


# Beam search decoding

@torch.no_grad()
def decode_beam(model: ASTacticModel, src_ids: torch.Tensor, beam_size: int = 5, max_len: int = 256) -> str:
    cfg = model.cfg
    device = next(model.parameters()).device
    PAD, BOS, EOS = cfg.pad_id, cfg.bos_id, cfg.eos_id

    src_ids = src_ids.to(device).unsqueeze(0)  # (1, L)
    mem = model.trf.encoder(model.src_pos(model.src_embed(src_ids)))
    src_key = (src_ids == PAD)

    beams: List[Tuple[torch.Tensor, float]] = [(torch.tensor([BOS], device=device), 0.0)]
    finished: List[Tuple[torch.Tensor, float]] = []

    for _ in range(max_len):
        new_beams: List[Tuple[torch.Tensor, float]] = []
        all_eos = True
        for seq, lp in beams:
            if seq[-1].item() == EOS:
                finished.append((seq, lp))
                continue
            all_eos = False
            t = model.tgt_pos(model.tgt_embed(seq.unsqueeze(0)))
            mask = torch.triu(torch.ones(t.size(1), t.size(1), device=device), 1).bool()
            dec = model.trf.decoder(t, mem, tgt_mask=mask, memory_key_padding_mask=src_key)
            logits = model.out_proj(dec[:, -1, :])  # (1, V)
            logprobs = torch.log_softmax(logits, dim=-1).squeeze(0)
            topk = torch.topk(logprobs, beam_size)
            for next_id, next_lp in zip(topk.indices, topk.values):
                new_seq = torch.cat([seq, next_id.view(1)], dim=0)
                new_beams.append((new_seq, lp + float(next_lp.item())))
        if not new_beams:
            break
        new_beams.sort(key=lambda x: x[1], reverse=True)
        beams = new_beams[:beam_size]
        if all_eos:
            finished.extend(beams)
            break

    if not finished:
        finished = beams
    best_seq, _ = max(finished, key=lambda x: x[1])

    # Convert to string (strip BOS/EOS)
    out_ids = []
    for tid in best_seq[1:]:
        t = int(tid.item())
        if t == EOS:
            break
        out_ids.append(t - 1)
    return bytes(out_ids).decode("utf-8", errors="ignore").strip()



# Lean checker

class LeanChecker:
    """
    Creates a temporary Lean file:
        import <imports...>
        example : <goal> := by
          <tactics>
    Compiles it with `lean`. Exit code 0 indicates a formally correct proof.
    """

    def __init__(self, default_imports: Optional[List[str]] = None, timeout_sec: float = 10.0):
        self.default_imports = default_imports or ["Std"]
        self.timeout = timeout_sec

    def verify(self, goal: str, tactic: str, imports: Optional[List[str]] = None) -> Tuple[bool, str]:
        imps = list(self.default_imports)
        if imports and isinstance(imports, (list, tuple)):
            imps += [x for x in imports if isinstance(x, str)]
        tactic = " ".join((tactic or "").split())

        code_lines = [f"import {m}" for m in imps]
        code_lines += [
            "",
            "set_option maxHeartbeats 200000",
            "set_option linter.unusedVariables false",
            "",
            f"example : {goal} := by",
            f"  {tactic}",
            "",
        ]
        code = "\n".join(code_lines)

        with tempfile.TemporaryDirectory() as td:
            fpath = os.path.join(td, "Check.lean")
            with open(fpath, "w", encoding="utf-8") as f:
                f.write(code)
            try:
                r = subprocess.run(["lean", fpath], capture_output=True, text=True, timeout=self.timeout)
                ok = (r.returncode == 0)
                err = r.stderr.strip() if not ok else ""
                return ok, err
            except subprocess.TimeoutExpired:
                return False, f"timeout>{self.timeout}s"

# Training / evaluation

def train_one_epoch(model, loader, optim, device="cuda", grad_clip=1.0) -> float:
    model.train()
    scaler = torch.amp.GradScaler("cuda", enabled=(device.startswith("cuda") and torch.cuda.is_available()))
    autocast_ctx = (lambda: torch.amp.autocast("cuda")) if (device.startswith("cuda") and torch.cuda.is_available()) else nullcontext

    total = 0.0
    steps = 0
    for step, batch in enumerate(loader):
        if batch["tgt_in_ids"] is None:
            continue  # skip rows without supervision

        src = batch["src_ids"].to(device, non_blocking=True)
        tgt_in = batch["tgt_in_ids"].to(device, non_blocking=True)
        tgt_out = batch["tgt_out_ids"].to(device, non_blocking=True)

        with autocast_ctx():
            logits = model(src, tgt_in)
            loss = model.loss(logits, tgt_out)

        optim.zero_grad(set_to_none=True)
        if scaler.is_enabled():
            scaler.scale(loss).backward()
            scaler.unscale_(optim)
            if grad_clip is not None and grad_clip > 0:
                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
            scaler.step(optim)
            scaler.update()
        else:
            loss.backward()
            if grad_clip is not None and grad_clip > 0:
                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
            optim.step()

        total += float(loss.item())
        steps += 1

    return total / max(1, steps)


@torch.no_grad()
def evaluate_loss(model, loader, device="cuda") -> float:
    model.eval()
    total = 0.0
    steps = 0
    for batch in loader:
        if batch["tgt_in_ids"] is None:
            continue
        src = batch["src_ids"].to(device)
        tgt_in = batch["tgt_in_ids"].to(device)
        tgt_out = batch["tgt_out_ids"].to(device)
        logits = model(src, tgt_in)
        total += float(model.loss(logits, tgt_out).item())
        steps += 1
    return total / max(1, steps)


# Verification loop
def run_verification(
    args,
    model: ASTacticModel,
    hf_ds,
    split_name: str,
    default_imports: List[str],
) -> Tuple[int, int, float]:
    ds = HFTacticDataset(
        hf_ds[split_name],
        goal_field=args.goal_field,
        gold_tactic_field=args.gold_tactic_field,
        imports_field=args.imports_field,
        max_src_len=args.max_src_len,
        max_tgt_len=args.max_tgt_len,
    )
    loader = DataLoader(
        ds,
        batch_size=1,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        collate_fn=lambda b: collate_tactics(b, max_tgt_len=args.max_tgt_len),
    )

    os.makedirs(args.save_dir, exist_ok=True)
    pred_path = os.path.join(args.save_dir, f"{args.run_name}_predict_{split_name}.jsonl")
    form_path = os.path.join(args.save_dir, f"{args.run_name}_formal_{split_name}.jsonl")

    pred_f = open(pred_path, "w", encoding="utf-8")
    form_f = open(form_path, "w", encoding="utf-8")

    checker = LeanChecker(default_imports=default_imports, timeout_sec=args.lean_timeout)

    total = 0
    ok = 0
    t0 = time.time()

    for batch in loader:
        total += 1
        meta = batch["meta"][0]
        gid = meta["id"]
        goal = meta["goal"]
        imports = meta["imports"]
        src_ids = batch["src_ids"][0]

        pred_tac = decode_beam(model, src_ids, beam_size=args.beam_size, max_len=args.max_tgt_len)

        pred_f.write(json.dumps({"id": gid, "goal": goal, "pred_tactic": pred_tac}, ensure_ascii=False) + "\n")

        success, err = checker.verify(goal, pred_tac, imports=imports)
        ok += int(success)
        form_f.write(json.dumps({
            "id": gid,
            "success": bool(success),
            "error": err,
        }, ensure_ascii=False) + "\n")

        if total % 50 == 0:
            rate = 100.0 * ok / total
            print(f"[verify:{split_name}] {total} examples | correct {ok} ({rate:.2f}%)")

    dt = time.time() - t0
    pred_f.close()
    form_f.close()

    rate = 100.0 * ok / max(1, total)
    print(f"[verify:{split_name}] Evaluated {total} | Formally-correct {ok} ({rate:.2f}%) | time={dt:.1f}s")
    return ok, total, dt


# Main
def main():
    args = build_args()
    set_seed(args.seed)

    device = args.device if (args.device.startswith("cuda") and torch.cuda.is_available()) else "cpu"
    os.makedirs(args.save_dir, exist_ok=True)

    default_imports = [s.strip() for s in args.default_imports.split(",") if s.strip()]

    print(f"[data] loading {args.data_root}")
    hf = load_from_disk(args.data_root)

    cfg = ASTacticConfig(
        src_vocab_size=259,
        tgt_vocab_size=259,
        d_model=args.d_model,
        nhead=args.nhead,
        num_layers=args.num_layers,
        dim_feedforward=args.ff_dim,
        dropout=args.dropout,
        max_src_len=args.max_src_len,
        max_tgt_len=args.max_tgt_len,
        pad_id=0,
        bos_id=257,
        eos_id=258,
        label_smoothing=args.label_smoothing,
    )
    model = ASTacticModel(cfg).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)

    ckpt_dir = os.path.join(args.save_dir, "ckpts")
    os.makedirs(ckpt_dir, exist_ok=True)
    best_ckpt = os.path.join(ckpt_dir, "best.pt")

    # Dataloaders (training and validation for loss-based early selection)
    if "train" in hf and ("train" in args.mode or "train+verify" in args.mode):
        train_ds = HFTacticDataset(
            hf["train"],
            goal_field=args.goal_field,
            gold_tactic_field=args.gold_tactic_field,
            imports_field=args.imports_field,
            max_src_len=args.max_src_len,
            max_tgt_len=args.max_tgt_len,
        )
        val_split_name = "validation" if "validation" in hf else ("test" if "test" in hf else None)
        val_loader = None
        if val_split_name:
            val_ds = HFTacticDataset(
                hf[val_split_name],
                goal_field=args.goal_field,
                gold_tactic_field=args.gold_tactic_field,
                imports_field=args.imports_field,
                max_src_len=args.max_src_len,
                max_tgt_len=args.max_tgt_len,
            )
            val_loader = DataLoader(
                val_ds,
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=args.num_workers,
                pin_memory=True,
                collate_fn=lambda b: collate_tactics(b, max_tgt_len=args.max_tgt_len),
            )

        train_loader = DataLoader(
            train_ds,
            batch_size=args.batch_size,
            shuffle=True,
            num_workers=args.num_workers,
            pin_memory=True,
            collate_fn=lambda b: collate_tactics(b, max_tgt_len=args.max_tgt_len),
        )

        best_val = float("inf")
        for ep in range(1, args.epochs + 1):
            t0 = time.time()
            tr = train_one_epoch(model, train_loader, optimizer, device=device, grad_clip=args.grad_clip)
            va = evaluate_loss(model, val_loader, device=device) if val_loader is not None else tr
            dt = time.time() - t0
            print(f"[epoch {ep:02d}] train {tr:.4f} | valid {va:.4f} | time {dt:.1f}s")
            if va < best_val:
                best_val = va
                torch.save({"model": model.state_dict(), "cfg": cfg.__dict__}, best_ckpt)
                print(f"[ckpt] saved best: {best_ckpt}")

    # Load checkpoint for verification
    if "verify" in args.mode:
        ckpt_to_load = args.load_ckpt if args.load_ckpt else best_ckpt
        if os.path.exists(ckpt_to_load):
            state = torch.load(ckpt_to_load, map_location=device)
            model.load_state_dict(state["model"])
            print(f"[ckpt] loaded: {ckpt_to_load}")
        else:
            print(f"[warn] checkpoint not found: {ckpt_to_load}; verifying with current weights")

        # Verify requested splits
        results_summary = []
        for split in args.verify_splits:
            if split not in hf:
                print(f"[verify] split missing in dataset: {split}")
                continue
            ok, total, secs = run_verification(args, model, hf, split, default_imports)
            results_summary.append((split, ok, total, secs))

        # Save summary CSV
        csv_path = os.path.join(args.save_dir, f"{args.run_name}_summary.csv")
        with open(csv_path, "w", newline="") as f:
            w = csv.writer(f)
            w.writerow(["split", "correct", "total", "accuracy_percent", "wall_time_sec"])
            for split, ok, total, secs in results_summary:
                acc = 100.0 * ok / max(1, total)
                w.writerow([split, ok, total, f"{acc:.2f}", f"{secs:.1f}"])
        print(f"[summary] {csv_path}")


if __name__ == "__main__":
    main()